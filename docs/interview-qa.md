# 面接 Q&A メモ

このプロジェクトについて面接で聞かれそうな質問と、簡潔な回答をまとめています。

---

## Q1. なぜスキル軸を 5 カテゴリ（Frontend / Backend / Infra / AI / Tools）にした？

**A.** 若手エンジニアが自分の強み・弱みを直感的に把握できて、かつ市場で求められる主要領域を過不足なくカバーできるバランスだったからです。  
特に「AI」と「Tools」は、モダンな開発において横断的に重要となるスキルとして独立させ、将来性や生産性への意識も示せるようにしました。

---

## Q2. 転職準備スコアを 40-30-20-10 にした理由は？

**A.** 一番土台となる技術力（Skill）を最大配点にしつつ、市場とのマッチ（Job）、リスク（Risk）、行動（Prep）の4つを偏りなく評価できるように重み付けしました。

- **SkillScore (0-40点)**: 技術力が最も基礎であるため、配点を最大に
- **JobScore (0-30点)**: 市場とのマッチング度合いも重要なため、次に高い配点
- **RiskScore (0-20点)**: 陳腐化・属人化リスクも考慮に入れ、長期的な視点を促す
- **PrepScore (0-10点)**: ポートフォリオ・面接練習などの準備状況は最後の後押し

これにより、単なる技術力だけでなく「市場価値」「リスク管理」「準備行動」という多角的な視点から転職準備度を評価できます。

---

## Q3. Usage ログや E2E を入れた狙いは？

**A.** 作って終わりではなく「どの機能がどれだけ使われているか」「主要なユーザーフローが壊れていないか」を継続的に確認できるようにし、プロダクトとして改善サイクルを回せることを示したかったからです。

- **Usage ログ**: `/admin/usage` で日別・機能別の利用状況を可視化。将来的に Prometheus / Grafana などと連携するための土台にもなる
- **E2E（Playwright）**: ホーム → 診断 → 結果というコアフローが壊れていないかを CI で自動検証
- **カバレッジレポート**: Vitest でユニットテストを回し、coverage を GitHub Actions Artifacts にアップロードして「テストを運用している」ことを示す

---

## Q4. AI のプロンプト設計で工夫した点は？

**A.** ハイブリッド評価とコスト効率を意識しました。

- **ハイブリッド評価**: ルールベース評価（文字数チェック、STAR要素の有無など）で検出した具体的な問題点を AI プロンプトに含めることで、AI の柔軟性とルールベースの安定性を両立
- **構造化された出力**: AI からのフィードバックは「良かった点」「改善点」「次の一歩」という 3 セクションで返すよう指示し、利用者が行動に移しやすい形式に
- **コスト効率**: `gpt-4o-mini` を基本モデルとし、`max_tokens` や `temperature` を調整して品質を保ちつつ API コストを最適化

---

## Q5. このプロジェクトで一番苦労した点は？

**A.** 「診断結果だけではユーザーは次の一歩を踏み出しにくい」という課題への対処です。

最初は「AI でスキルを診断するだけのツール」として構想していましたが、診断結果を見せるだけではユーザーが何をすればいいか分からないことに気づきました。  
そこで、単なる診断に留まらず：

- **学習ロードマップ**: 具体的な学習計画を提示
- **求人マッチング**: 市場とのギャップを可視化
- **面接練習**: アウトプットの場を提供
- **ダッシュボード**: 継続的な成長を可視化

という「診断 → 理解 → 行動 → 成長」のサイクルを回せるプロダクトへと進化させました。

---

## Q6. 今後の改善予定は？

**A.** 主に以下を検討しています：

1. **パフォーマンス改善**: Lighthouse スコアの継続監視、Critical CSS のインライン化
2. **アクセシビリティ強化**: Axe DevTools による定期監査、スクリーンリーダーでのユーザーテスト
3. **AI 機能の拡張**: Azure OpenAI / Google Vertex AI 対応、カスタムルール定義
4. **チーム開発想定の機能**: GitHub Discussions でのフィードバック収集、コントリビューター向けドキュメント整備


